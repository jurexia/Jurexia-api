"""
Iurexia Legal Cache Manager â€” ON-DEMAND Strategy (v7 â€” Feb 2026)
================================================================
Creates a Gemini cached context (~950K tokens of Mexican legal texts)
ONLY when a user explicitly activates "Genio JurÃ­dico" â€” NOT at server startup.

API: Gemini AI Studio (google-genai SDK) via GEMINI_API_KEY
Model: gemini-2.0-flash-001 (or configured via CACHE_MODEL env var)
Corpus: 12 files in cache_corpus/ â€” CPEUM, CCF, CPF, LFT, Amparo, CCom, LGTOC + 5 DDHH treaties
Limit: ~1,048,576 tokens HARD LIMIT â€” corpus capped at 950,000 tokens

SAFETY LOCKS (9 total):
  1. Orphan Cleanup â€” deletes ALL existing caches before creating a new one
  2. asyncio.Lock â€” prevents concurrent creation (race conditions)
  3. Double-check â€” re-verifies inside the lock
  4. TTL 8 min â€” auto-expires in Gemini if unused
  5. Frontend Timer â€” UI disables button after 8 min inactivity
  6. Kill Switch â€” /genio/kill endpoint for emergencies
  7. Token Limit â€” ABORTS if corpus exceeds MAX_CACHE_TOKENS
  8. Budget Guard â€” max 10 cache creates per day
  9. Startup Cleanup â€” lifespan ONLY DELETES, NEVER creates

Usage:
    cache_name = await get_or_create_cache()
"""

import os
import time
import asyncio
import logging
import datetime
from pathlib import Path
from typing import Optional

logger = logging.getLogger("iurexia.cache")

# â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CACHE_MODEL = os.getenv("CACHE_MODEL", "gemini-2.0-flash-001")
CACHE_CORPUS_DIR = os.getenv("CACHE_CORPUS_DIR", "cache_corpus")
CACHE_TTL_MINUTES = int(os.getenv("CACHE_TTL_MINUTES", "8"))
CACHE_DISPLAY_NAME = "iurexia-legal-corpus-v7"
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")

# â”€â”€ Safety Limits â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MAX_CACHE_TOKENS = 950_000       # HARD SAFETY: abort if corpus exceeds this
MAX_DAILY_CREATES = int(os.getenv("MAX_DAILY_CREATES", "10"))  # Budget guard

# â”€â”€ Global State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_cache_name: Optional[str] = None
_cache_created_at: float = 0
_cache_lock = None  # Initialized lazily in get_or_create_cache
_gemini_client = None
_last_error: Optional[str] = None
_daily_create_count: int = 0
_daily_create_date: str = ""  # YYYY-MM-DD


def get_gemini_client():
    """Get or create the shared Gemini AI Studio client (singleton)."""
    global _gemini_client
    if _gemini_client is None:
        if not GEMINI_API_KEY:
            raise RuntimeError("GEMINI_API_KEY is not set â€” cannot create Gemini client")
        from google import genai
        logger.info("Initializing Gemini Client via AI Studio (GEMINI_API_KEY)")
        _gemini_client = genai.Client(api_key=GEMINI_API_KEY)
    return _gemini_client


def _load_corpus_texts() -> list[str]:
    """Load all TXT files from the corpus directory, sorted by name.

    SAFETY LOCK #7: Validates total tokens < MAX_CACHE_TOKENS.
    Returns empty list if exceeded (prevents oversized cache creation).
    """
    global _last_error
    corpus_dir = Path(CACHE_CORPUS_DIR)
    if not corpus_dir.exists():
        _last_error = f"Corpus dir not found: {corpus_dir.resolve()} (cwd: {Path.cwd()})"
        logger.error(f"ðŸš¨ {_last_error}")
        return []

    texts = []
    files = sorted(corpus_dir.glob("*.txt"))
    if not files:
        _last_error = f"No .txt files found in {corpus_dir.resolve()}"
        logger.error(f"ðŸš¨ {_last_error}")
        return []

    for f in files:
        try:
            text = f.read_text(encoding="utf-8")
            texts.append(text)
            tokens_est = len(text) // 4
            logger.info(f"  Loaded {f.name}: {len(text):,} chars (~{tokens_est:,} tokens)")
        except Exception as e:
            logger.error(f"  Failed to load {f.name}: {e}")

    total_chars = sum(len(t) for t in texts)
    total_tokens = total_chars // 4
    logger.info(f"  Total corpus: {len(texts)} files, {total_chars:,} chars (~{total_tokens:,} tokens)")

    # SAFETY LOCK #7: Token limit validation
    if total_tokens > MAX_CACHE_TOKENS:
        _last_error = f"Corpus too big: {total_tokens:,} > {MAX_CACHE_TOKENS:,} tokens â€” ABORTING"
        logger.error(f"ðŸš¨ {_last_error}")
        return []

    return texts


async def _cleanup_orphan_caches():
    """SAFETY LOCK #1: Delete ALL existing caches before creating a new one.

    Prevents cache multiplication when Render restarts/redeploys.
    """
    try:
        client = get_gemini_client()
        caches = list(client.caches.list())
        orphans = [c for c in caches if getattr(c, "display_name", "") == CACHE_DISPLAY_NAME]

        if orphans:
            logger.warning(f"Found {len(orphans)} orphan cache(s) â€” deleting...")
            for cache in orphans:
                try:
                    client.caches.delete(name=cache.name)
                    logger.info(f"  Deleted orphan: {cache.name}")
                except Exception as e:
                    logger.error(f"  Failed to delete orphan {cache.name}: {e}")
        else:
            logger.info("No orphan caches found.")
    except Exception as e:
        logger.error(f"Orphan cleanup failed: {e}")


def _check_daily_budget() -> bool:
    """SAFETY LOCK #8: Budget guard â€” max N cache creates per calendar day."""
    global _daily_create_count, _daily_create_date
    today = datetime.date.today().isoformat()
    if _daily_create_date != today:
        _daily_create_count = 0
        _daily_create_date = today
    if _daily_create_count >= MAX_DAILY_CREATES:
        logger.error(
            f"ðŸš¨ BUDGET GUARD: {_daily_create_count} cache creates today "
            f"(max={MAX_DAILY_CREATES}). BLOCKING new cache creation."
        )
        return False
    return True


async def _create_cache() -> Optional[str]:
    """Create a new Gemini context cache with the legal corpus.

    Always cleans up orphans first. Validates token count and daily budget.
    """
    global _cache_name, _cache_created_at, _daily_create_count, _last_error

    try:
        # SAFETY LOCK #8
        if not _check_daily_budget():
            _last_error = f"Daily cache creation limit reached ({MAX_DAILY_CREATES}/dÃ­a)"
            return None

        # SAFETY LOCK #1: Clean up orphans first
        await _cleanup_orphan_caches()

        from google.genai import types as gtypes

        client = get_gemini_client()

        # SAFETY LOCK #7: Load and validate corpus
        texts = _load_corpus_texts()
        if not texts:
            if not _last_error:
                _last_error = f"No corpus texts loaded from '{CACHE_CORPUS_DIR}'"
            logger.error(f"Cannot create cache: {_last_error}")
            return None

        # Build cache contents (user turn with all legal texts)
        contents = [
            gtypes.Content(
                role="user",
                parts=[gtypes.Part(text=t) for t in texts]
            )
        ]

        ttl_seconds = CACHE_TTL_MINUTES * 60

        system_instruction = (
            "Eres Iurexia, un asistente jurÃ­dico mexicano de Ã©lite. "
            "Tienes acceso directo al texto Ã­ntegro de las siguientes leyes y tratados "
            "internacionales ratificados por MÃ©xico. Cuando el usuario haga una consulta legal, "
            "cita TEXTUALMENTE los artÃ­culos relevantes con su nÃºmero exacto y ley de origen. "
            "Nunca inventes contenido legal. Si un artÃ­culo no estÃ¡ en tu contexto, dilo explÃ­citamente."
        )

        logger.info(
            f"Creating Gemini context cache ON-DEMAND: model={CACHE_MODEL}, "
            f"ttl={CACHE_TTL_MINUTES}m, files={len(texts)}"
        )

        # Create the cache â€” NO tools (not supported in cached content)
        cache = await client.aio.caches.create(
            model=CACHE_MODEL,
            config=gtypes.CreateCachedContentConfig(
                display_name=CACHE_DISPLAY_NAME,
                system_instruction=system_instruction,
                contents=contents,
                ttl=f"{ttl_seconds}s",
            )
        )

        _cache_name = cache.name
        _cache_created_at = time.time()
        _daily_create_count += 1
        _last_error = None  # Clear any previous error on success

        logger.info(
            f"âœ… Cache created: {_cache_name} "
            f"(TTL={CACHE_TTL_MINUTES}m, daily={_daily_create_count}/{MAX_DAILY_CREATES})"
        )
        return _cache_name

    except Exception as e:
        _last_error = f"{type(e).__name__}: {e}"
        logger.error(f"âŒ Cache creation failed: {_last_error}")
        import traceback
        logger.error(traceback.format_exc())
        return None


def _is_cache_valid() -> bool:
    """Check if the current cache is still within its TTL."""
    if not _cache_name:
        return False
    elapsed = time.time() - _cache_created_at
    ttl_seconds = CACHE_TTL_MINUTES * 60
    return elapsed < (ttl_seconds * 0.98)  # 98% of TTL for safety margin


async def _refresh_cache_ttl(cache_name: str):
    """Background task to extend the TTL of an active cache."""
    try:
        from google.genai import types as gtypes
        client = get_gemini_client()
        ttl_seconds = CACHE_TTL_MINUTES * 60
        await client.aio.caches.update(
            name=cache_name,
            config=gtypes.UpdateCachedContentConfig(ttl=f"{ttl_seconds}s")
        )
        logger.info(f"  Cache TTL refreshed: {cache_name}")
    except Exception as e:
        logger.error(f"  Failed to refresh cache TTL: {e}")


async def get_or_create_cache() -> Optional[str]:
    """Main entry point: get existing cache or create one on-demand.

    Fast path if cache exists and is valid.
    Uses asyncio.Lock (SAFETY LOCK #2) with double-check (#3) to prevent races.
    """
    global _cache_lock, _cache_created_at

    # Fast path
    if _is_cache_valid():
        _cache_created_at = time.time()
        asyncio.create_task(_refresh_cache_ttl(_cache_name))
        return _cache_name

    # Ensure lock exists
    if _cache_lock is None:
        _cache_lock = asyncio.Lock()

    async with _cache_lock:
        # Double-check inside lock (#3)
        if _is_cache_valid():
            _cache_created_at = time.time()
            asyncio.create_task(_refresh_cache_ttl(_cache_name))
            return _cache_name

        return await _create_cache()


def get_cache_name() -> Optional[str]:
    """Get current active cache name (sync)."""
    global _cache_created_at
    if _is_cache_valid():
        _cache_created_at = time.time()
        return _cache_name
    return None


async def get_cache_name_async() -> Optional[str]:
    """Get or create cache lazily (async)."""
    return await get_or_create_cache()


def get_cache_model() -> str:
    """Return the model to use for cached requests."""
    return CACHE_MODEL


def get_cache_status() -> dict:
    """Return cache status for diagnostics and the /genio/status endpoint."""
    elapsed = time.time() - _cache_created_at if _cache_created_at else 0
    ttl_seconds = CACHE_TTL_MINUTES * 60
    return {
        "cache_name": _cache_name,
        "cache_model": CACHE_MODEL,
        "cache_available": _is_cache_valid(),
        "cache_age_minutes": round(elapsed / 60, 1) if elapsed else 0,
        "cache_ttl_minutes": CACHE_TTL_MINUTES,
        "cache_remaining_minutes": round(max(0, ttl_seconds - elapsed) / 60, 1) if elapsed else 0,
        "corpus_dir": CACHE_CORPUS_DIR,
        "strategy": "on-demand",
        "est_hourly_cost": "$0.09" if _is_cache_valid() else "$0.00",
        "last_error": _last_error,
        "daily_creates": f"{_daily_create_count}/{MAX_DAILY_CREATES}",
        "max_tokens_limit": MAX_CACHE_TOKENS,
    }


async def cleanup_on_startup():
    """SAFETY LOCK #9: Called from app lifespan â€” ONLY deletes, NEVER creates.

    Ensures server restarts don't leave orphan caches running at cost.
    """
    logger.info("Startup cleanup: checking for orphan caches...")
    await _cleanup_orphan_caches()
    logger.info("Startup cleanup complete â€” no cache created (on-demand only)")


async def delete_all_caches():
    """Emergency kill switch: delete ALL caches immediately."""
    global _cache_name, _cache_created_at
    await _cleanup_orphan_caches()
    _cache_name = None
    _cache_created_at = 0
    logger.info("All caches deleted via kill switch")
